{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYqqG4qcp7oB"
   },
   "source": [
    "# Семинар 8: эмбеддинги слов\n",
    "\n",
    "## Вступление\n",
    "Сегодня мы начинаем работать с текстами. Первый шаг любого пайплайна для обработки текстов на естественных языках (NLP, natural language processing) — это векторизация текстов или их составляющих (буквосочетаний, слов, словосочетаний). Иными словами, перевод текстов из формы последовательности букв/слов/токенов в числовые векторы. Такие векторы обычно называют **эмбеддингами**. Для задач NLP (part of speech tagging, named entity recognition, генерация текста, etc.) бывает полезно пользоваться готовыми эмбеддингами, полученными за нас. Далее, при решении конкретной задачи, слова в текстах заменяют готовыми эмбеддингами и поверх этого дела уже строят разные модели.\n",
    "\n",
    "<img src='https://github.com/udacity/deep-learning-v2-pytorch/blob/master/word2vec-embeddings/assets/one_hot_encoding.png?raw=1' width=40%>\n",
    "\n",
    "Конечно же, как мы это делали в предыдущем курсе, можно использовать one-hot кодирование для каждого слова. Далее one-hot векторы можно обрабатывать обычным линейным слоем, обучать его как часть модели и получать, в целом, похожую систему. Такой подход выливается в целый ряд проблем:\n",
    "- Во-первых, умножение строки в виде one-hot вектора на матрицу линейного слоя можно заменить на простую индексацию строки матрицы этого слоя.\n",
    "- Во-вторых, мы можем быть заинтересованы в хорошо обученных эмбеддингах на большом датасете (чтобы векторные представления хорошо отражали смысл слов), а в нашей конкретной задаче структура и/или количество данных могут отличаться.\n",
    "- В-третьих, если у нас есть основания полагать, что готовые эмбеддинги хорошо подходят для решаемой задачи, то мы можем немного сэкономить на обучении очень большого линейного слоя, ведь матрица эмбеддинигов имеет размеры `vocab_size x embed_size` и для стандартного словаря (десятки тысяч слов) может занимать больше 10 мегабайт памяти.\n",
    "\n",
    "<img src='https://github.com/udacity/deep-learning-v2-pytorch/blob/master/word2vec-embeddings/assets/lookup_matrix.png?raw=1' width=60%>\n",
    "\n",
    "Подведём небольшие итоги. Наша цель — получить векторные представления одинакового размера для каждого слова из словаря. При этом перед обработкой слов из текста моделью, мы хотим по номеру слова в словаре брать из таблицы эмбеддингов нужный вектор и далее работать с ним как с признаковым описанием слова.\n",
    "\n",
    "<img src='https://github.com/udacity/deep-learning-v2-pytorch/blob/master/word2vec-embeddings/assets/tokenize_lookup.png?raw=1' width=40%>\n",
    "\n",
    "Конечно эмбеддинги используются не только для слов. Эмбеддингом называют любую векторную репрезентацию дискретных объектов: слов (или частей слов), пользователей сервиса и чего только не. Для того чтобы хорошенько со всем этим разобраться, мы реализуем один из самых известных подходов к построению эмбеддингов слов — Word2Vec.\n",
    "\n",
    "### Полезные ссылки\n",
    "* [Общий взгляд](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) на Word2Vec\n",
    "* [Первые работы](https://arxiv.org/pdf/1301.3781.pdf) с Word2Vec\n",
    "* [Neural IPS, paper](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) с улучшениями Word2Vec\n",
    "\n",
    "### План семинара\n",
    "1. Повторяем теорию про Word2Vec\n",
    "2. Скачиваем и преодобрабатываем данные\n",
    "3. Обучаем модель Skip-Gram\n",
    "4. Анализируем обученную модель\n",
    "\n",
    "Приступим!\n",
    "\n",
    "---\n",
    "## 1. Повторяем теорию про Word2Vec\n",
    "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/word2vec-embeddings/assets/context_drink.png?raw=1\" width=40%>\n",
    "\n",
    "Word2Vec позволит нам не просто получить какие-то векторы чисел, соответствующие словам, а ещё и сохранить семантику слов в этих векторах. Занимательное свойство про семантику слов проявляется в том, что в полученном пространстве эмбеддингов векторы, соответствующие словам с близкими смыслами, будут иметь маленькое расстояние друг между другом. Такие слова, как «кофе», «чай» и «вода», появляются в похожих **контекстах**, а значит будут иметь близкие векторы. Различные слова будут дальше друг от друга, а отношения могут быть представлены расстоянием в векторном пространстве. Построение подхода, удовлетворяющего такому свойству, отчасти отражает наше понимание слов, ведь при прочтении неизвестного слова в *достаточном* количестве контекстов, мы учимся понимать его смысл.\n",
    "\n",
    "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/word2vec-embeddings/assets/vector_distance.png?raw=1\" width=40%>\n",
    "\n",
    "Для реализации Word2Vec существует два подхода:\n",
    "* **CBOW** (Continuous Bag-Of-Words). По контексту (словам вокруг) слова пытаемся предсказать центральное слово.\n",
    "* **Skip-gram**. По центральному слову контекста пытаемся предсказать слова из контекста.\n",
    "\n",
    "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/word2vec-embeddings/assets/word2vec_architectures.png?raw=1\" width=60%>\n",
    "\n",
    "На этом семинаре мы будем использовать **архитектуру skip-gram**, потому что она работает лучше чем CBOW.\n",
    "\n",
    "## 2. Скачиваем и преодобрабатываем данные\n",
    "\n",
    "1. Скачаем [dataset](https://s3.amazonaws.com/video.udacity-data.com/topher/2018/October/5bbe6499_text8/text8.zip); файл очищенного текста статьи в Википедии от Мэтта Махони\n",
    "2. Очистим данные от шума\n",
    "3. Сделаем маппинг слов в индексы\n",
    "4. Проведём субдискретизацию текста\n",
    "5. Сформируем батчи\n",
    "\n",
    "### Качаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YkuGJuoqAFKs",
    "outputId": "e202b134-1f85-4d20-a181-6cb6c1722d82",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !wget https://s3.amazonaws.com/video.udacity-data.com/topher/2018/October/5bbe6499_text8/text8.zip\n",
    "# !unzip text8.zip\n",
    "# !rm text8.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o65mAtMVADv5",
    "outputId": "a6525cee-53e6-441a-95be-03fd82368b64"
   },
   "outputs": [],
   "source": [
    "with open(\"text8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ck6w80Q5p7oD"
   },
   "source": [
    "### Чистим данные\n",
    "\n",
    "Теперь почистим текст, чтобы облегчить обучение. Вся логика реализована за нас в функции `preprocess` из файла `utils.py`. Функция делает несколько вещей:\n",
    "* Преобразует любые знаки препинания в токены, поэтому точка заменяется на `<PERIOD>`. В этом наборе данных нет никаких периодов, но это поможет при решении других задач. \n",
    "* Удаляет все слова, которые встречаются в наборе данных пять или меньше раз. Это значительно уменьшит проблемы, связанные с шумом в данных, и улучшит качество векторных представлений.\n",
    "* Возвращает список слов в тексте.\n",
    "\n",
    "Это может занять несколько секунд, так как наш текстовый файл довольно большой. Если вы хотите написать свои собственные функции для этого материала, дерзайте!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aFUmJtPup7oD",
    "outputId": "86859698-2a99-4d3f-efdf-aaa9bdca7ea7"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "\n",
    "words = utils.preprocess(text)\n",
    "print(f\"Beginning of the text: {words[:30]}\")\n",
    "print(f\"Total words in text: {len(words)}\")\n",
    "print(f\"Unique words: {len(set(words))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yC2mLtwip7oD"
   },
   "source": [
    "### Делаем маппинг слов в индексы\n",
    "\n",
    "Затем мы создаем два словаря для преобразования слов в целые числа и обратно. Это тоже делается с помощью функции из файла `utils.py`: `create_lookup_tables` принимает список слов в тексте и возвращает два словаря. Целые числа присваиваются в порядке убывания частоты, поэтому самому частому слову («the») присваивается число $0$, следующему по частоте — $1$ и так далее. \n",
    "\n",
    "Когда у нас есть словари, слова преобразуются в целые числа и сохраняются в списке `int_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05eK3RZSp7oD",
    "outputId": "20aa95a5-92ee-4640-87ff-e647041c061b"
   },
   "outputs": [],
   "source": [
    "vocab_to_int, int_to_vocab = utils.create_lookup_tables(words)\n",
    "int_words = [vocab_to_int[word] for word in words]\n",
    "\n",
    "print(int_words[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5HDfby5p7oD"
   },
   "source": [
    "#### Проводим субдискретизацию (subsampling)\n",
    "\n",
    "Часто встречающиеся слова (например \"the\", \"of\", \"for\", etc.) не обеспечивают особого контекста для близлежащих слов. Если мы отбросим некоторые из них, мы сможем удалить часть шума из наших данных и взамен получить более быстрое обучение и лучшее представление. Этот процесс иногда называют субдискретизацией. Для каждого слова $w_i$ в обучающем наборе мы отбрасываем его с вероятностью, равной\n",
    "\n",
    "$$ P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}}, $$\n",
    "\n",
    "где $t$ - пороговый параметр, а $f(w_i)$ - частота слова $w_i$ в общем наборе данных.\n",
    "\n",
    "$$ P(0) = 1 - \\sqrt{\\frac{1*10^{-5}}{10^6/(16*10^6)}} = 0.98735.$$\n",
    "\n",
    "#### Задание\n",
    "Реализуйте подвыборку для слов в `int_words`. Пройдите через `int_words` и отбросьте каждое слово с вероятностью $ P (w_i) $, показанной выше. Обратите внимание, что $ P (w_i) $ — это вероятность того, что слово будет отброшено. Сохраните отфильтрованные данные в переменной `train_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EenbWXACp7oD",
    "outputId": "11704915-5618-429b-9b32-b65c8f8fc93f"
   },
   "outputs": [],
   "source": [
    "threshold = 1e-5\n",
    "word_counts = Counter(int_words)  # dictionary with number of appearances for each word\n",
    "print(f\"42-th word appears in the text {word_counts[42]} times\")  \n",
    "\n",
    "# discard some frequent words, according to the subsampling method\n",
    "# create a new list of words for training\n",
    "\n",
    "train_words = []\n",
    "\n",
    "for int_w in tqdm(int_words):\n",
    "    if random.random() < (threshold / (word_counts[int_w] / len(int_words))) ** 0.5:\n",
    "        train_words.append(int_w)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(int_words[:15])\n",
    "print(train_words[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLCZzZVrp7oD"
   },
   "source": [
    "### Формируем обучающие пары\n",
    "После препроцессинга данных надо правильно сформировать обучающие примеры. В архитектуре skip-gram для каждого слова в тексте нужно определить окружающий _context_ и захватить все слова в окне вокруг этого слова с размером $C$.\n",
    "\n",
    "Из [статьи](https://arxiv.org/pdf/1301.3781.pdf): *поскольку более далекие слова обычно меньше связаны с текущим словом, чем близкие к нему, мы придаем меньший вес удаленным словам, отбирая меньшее количество из этих слов в наших обучающих примерах ... Если мы выберем $C = 5$, то для каждого обучающего слова мы выбираем случайным образом число $R$ в диапазоне $[1:C]$, а затем используем $R$ предыдущих слов и $R$ следующих слов в качестве правильных меток.*\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Задание\n",
    "Реализуйте функцию `get_target`, которая получает список слов, индекс и размер окна, а затем возвращает список слов в окне вокруг индекса. Обязательно используйте алгоритм, описанный выше, где вы выбрали случайное количество слов из окна.\n",
    "\n",
    "Скажем, у нас есть вход и нас интересует токен `idx = 2`, `741`: \n",
    "```\n",
    "[5233, 58, 741, 10571, 27349, 0, 15067, 58112, 3580, 58, 10712]\n",
    "```\n",
    "\n",
    "Для R = 2 функция get_target должна возвращать список из четырех значений:\n",
    "```\n",
    "[5233, 58, 10571, 27349]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LmzEBIfdp7oD"
   },
   "outputs": [],
   "source": [
    "def get_target(words: List[int], idx: int, window_size: int = 5) -> List[int]:\n",
    "    \"\"\"\n",
    "    Get a list of words in a random-sized window around an index.\n",
    "\n",
    "    :param words: a text represented as a sequence of words indices\n",
    "    :param idx: index of the central word that is used to make a batch\n",
    "    :param window_size: controls the size of the window for each word\n",
    "    :return: list of words in a window of a window_size size\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "259vdIDVp7oD",
    "outputId": "013606c4-2498-4a2d-e36e-27b82bedb2d5"
   },
   "outputs": [],
   "source": [
    "# test your code!\n",
    "# run this cell multiple times to check for random window selection\n",
    "# you should get some indices around the idx\n",
    "\n",
    "int_text = [i for i in range(10)]\n",
    "idx = 5\n",
    "target = get_target(int_text, idx=idx, window_size=5)\n",
    "print(\"Input: \", int_text)\n",
    "print(f\"Index of interest: {idx}\")\n",
    "print(\"Target: \", target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucXKA7zxp7oD"
   },
   "source": [
    "### Генерируем батчи\n",
    "\n",
    "В следующей ячейке реализована функция-генератор, которая возвращает батчи обучающих пар, используя описанную выше функцию get_target. Идея этой имплементации следующая: берём `batch_size` центральных слов, для каждого набираем соседей и называем это всё батчом. Обратите внимание, что настоящий размер батча в таком случае будет вариьироваться в отрезке чисел `[batch_size:batch_size * window_size]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HghTPaSgp7oD"
   },
   "outputs": [],
   "source": [
    "def get_batches(words: List[int], batch_size: int, window_size: int = 5) -> Tuple[List[int], List[int]]:\n",
    "    \"\"\"\n",
    "    Create a generator of word batches as a tuple (inputs, targets)\n",
    "    \"\"\"\n",
    "    for i in range(0, len(words) // batch_size * batch_size, batch_size):\n",
    "        x, y = [], []\n",
    "        batch = words[i:i + batch_size]\n",
    "        for j in range(len(batch)):\n",
    "            batch_x = batch[j]\n",
    "            batch_y = get_target(words, i + j, window_size)\n",
    "            y.extend(batch_y)\n",
    "            x.extend([batch_x] * len(batch_y))\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1TJ2c2XDp7oD",
    "outputId": "87ddcf50-f9ac-4cba-a614-12cb59c9d73c"
   },
   "outputs": [],
   "source": [
    "int_text = [5233, 3080, 11, 5, 194, 1, 3133, 45, 58]\n",
    "batch_gen = get_batches(int_text, batch_size=4, window_size=5)\n",
    "x, y = next(batch_gen)\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y: {y}\\n\")\n",
    "\n",
    "x, y = next(batch_gen)\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y: {y}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eeqYgmh2p7oD"
   },
   "source": [
    "## 3. Обучаем модель Skip-Gram\n",
    "Ниже представлена примерная схема общей структуры нашей сети:\n",
    "\n",
    "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/word2vec-embeddings/assets/skip_gram_arch.png?raw=1\" width=60%>\n",
    "\n",
    "* Входные слова передаются как батчи индексов входных слов. Целевые переменные для каждого входного слова — номер слова из контекста.\n",
    "* Батчи индексов входных слов обрабатываются линейным слоем `vocab_size x embed_size`. \n",
    "* Полученные эмбеддинги обрабатываются выходным линейным слоем размера `embed_size x vocab_size` и к выходам применяется лосс для задачи классификации.\n",
    "\n",
    "Идея в том, чтобы обучить матрицу весов слоя эмбеддингов и найти эффективные представления для наших слов. После обучения мы можем отбросить слой softmax, потому что нам не нужно делать прогнозы с помощью этой сети. Нам просто нужна матрица эмбеддингов, чтобы мы могли использовать ее в _других_ сетях, которые мы будем строить с использованием этого набора данных.\n",
    "\n",
    "### Validation\n",
    "\n",
    "Начнём с контроля обучения — валидации. Нужно выбрать несколько общих слов и несколько необычных слов. Затем мы распечатаем ближайшие к ним слова, используя косинусное сходство:\n",
    "\n",
    "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/word2vec-embeddings/assets/two_vectors.png?raw=1\" width=30%>\n",
    "\n",
    "$$\n",
    "\\mathrm{similarity} = \\cos(\\theta) = \\frac{\\vec{a} \\cdot \\vec{b}}{|\\vec{a}||\\vec{b}|}\n",
    "$$\n",
    "\n",
    "Мы можем закодировать слова проверки как векторы $\\vec{a}$, используя таблицу эмбеддингов, а затем вычислить сходство с каждым вектором слов $\\vec{b}$ в таблице эмбеддингов. Имея сходство, мы можем распечатать проверочные слова и слова в нашей матрице эмбеддингов, семантически похожие на эти слова. Это хороший способ проверить, объединяет ли наша таблица эмбеддингов слова с похожими семантическими значениями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GCEQG5RJp7oD"
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(\n",
    "    embedding: nn.Module, \n",
    "    valid_size: int = 16, \n",
    "    valid_window: int = 100, \n",
    "    device: str = \"cpu\"\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Computes cosine similarity between validation words and words in the embedding matrix.\n",
    "    \n",
    "    :param embedding: instance of torch.nn.Embedding module\n",
    "    :param valid_size: number of words to find closest words to\n",
    "    :param valid_window: number of words to draw examples from\n",
    "    :param device: device to execute computations on\n",
    "    :return: tensor of validation examples indices and tensor similarities to closest words\n",
    "    \"\"\"\n",
    "    \n",
    "    # Here we're calculating the cosine similarity between some random words and \n",
    "    # our embedding vectors. With the similarities, we can look at what words are\n",
    "    # close to our random words.\n",
    "    \n",
    "    # sim = (a . b) / |a||b|\n",
    "    \n",
    "    embed_vectors = embedding.weight\n",
    "    \n",
    "    # magnitude of embedding vectors, |b|\n",
    "    magnitudes = embed_vectors.pow(2).sum(dim=1).sqrt().unsqueeze(0)\n",
    "    \n",
    "    # pick N words from ranges (0, window) and (1000, 1000 + window). lower id implies more frequent \n",
    "    valid_examples = np.array(random.sample(range(valid_window), valid_size // 2))\n",
    "    valid_examples = np.append(valid_examples,\n",
    "                               random.sample(range(1000, 1000 + valid_window), valid_size // 2))\n",
    "    valid_examples = torch.LongTensor(valid_examples).to(device)\n",
    "    \n",
    "    valid_vectors = embedding(valid_examples)\n",
    "    similarities = torch.mm(valid_vectors, embed_vectors.t()) / magnitudes\n",
    "        \n",
    "    return valid_examples, similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "guJsEYdBp7oD"
   },
   "source": [
    "#### Задание\n",
    "\n",
    "Определите и обучите модель SkipGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W8h3wgY_p7oD"
   },
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_size: int):\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # YOUR CODE HERE\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fg9IEgfZp7oD",
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print_every = 1000\n",
    "steps = 0\n",
    "n_epochs = 10\n",
    "batch_size = 1024\n",
    "embedding_dim = 128\n",
    "\n",
    "model = SkipGram(len(vocab_to_int), embedding_dim)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "for e in trange(n_epochs, leave=True, desc=\"Epoch number\"):\n",
    "    pbar = tqdm(\n",
    "        get_batches(train_words, batch_size), \n",
    "        leave=False, \n",
    "        desc=\"Batch number\", \n",
    "        total=len(train_words) // batch_size\n",
    "    )\n",
    "    \n",
    "    # get input and target batches\n",
    "    for inputs, targets in pbar:\n",
    "        steps += 1\n",
    "        inputs, targets = torch.LongTensor(inputs), torch.LongTensor(targets)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        log_ps = model(inputs)\n",
    "        loss = criterion(log_ps, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if steps % print_every == 0:\n",
    "            # getting examples and similarities\n",
    "            valid_examples, valid_similarities = cosine_similarity(model.embed, device=device)\n",
    "            _, closest_idxs = valid_similarities.topk(6)\n",
    "            \n",
    "            valid_examples, closest_idxs = valid_examples.to(\"cpu\"), closest_idxs.to(\"cpu\")\n",
    "            for ii, valid_idx in enumerate(valid_examples):\n",
    "                closest_words = [int_to_vocab[idx.item()] for idx in closest_idxs[ii]][1:]\n",
    "                print(int_to_vocab[valid_idx.item()] + \" | \" + \", \".join(closest_words))\n",
    "            print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkpRD4A8p7oD"
   },
   "source": [
    "## 4. Анализируем обученную модель\n",
    "\n",
    "Ниже мы будем использовать T-SNE, чтобы визуализировать, как наши многомерные словесные векторы группируются вместе. Прочитайте [эту статью](http://colah.github.io/posts/2014-10-Visualizing-MNIST/), чтобы узнать больше о T-SNE и других способах визуализации многомерных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JoYaca7xp7oD",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bs-ZhlbPp7oD",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# getting embeddings from the embedding layer of our model, by name\n",
    "embeddings = model.embed.weight.to(\"cpu\").data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "za8ne2ADp7oE",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "viz_words = 600\n",
    "tsne = TSNE()\n",
    "embed_tsne = tsne.fit_transform(embeddings[:viz_words, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V1oitQiqp7oE",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "for idx in range(viz_words):\n",
    "    plt.scatter(*embed_tsne[idx, :], color=\"steelblue\")\n",
    "    plt.annotate(int_to_vocab[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=0.7)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Verteeva_Word2Vec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
