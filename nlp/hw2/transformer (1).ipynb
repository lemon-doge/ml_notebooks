{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nn.BatchNorm1d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def abs(x):\n",
    "    if x > 0:\n",
    "        return x\n",
    "    else:\n",
    "        return -x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.jit.ScriptFunction"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(jitted_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from einops.layers.torch import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jitted_abs(-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8z/rdxqfjbs0xx8flpxsgkvkxgw0000gn/T/ipykernel_34378/3622367031.py:2: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if x > 0:\n"
     ]
    }
   ],
   "source": [
    "jitted_abs = torch.jit.trace(abs, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# np.einsum('b n d, b m d -> b n m', x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PreNorm(nn.Module):\n",
    "    \"\"\"Apply layer normalization to the input and pass it through the layer.\"\"\"\n",
    "    def __init__(self, dim: int, layer: nn.Module) -> None:\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "    \n",
    "    def forward(self, x: torch.FloatTensor, **kwargs) -> torch.FloatTensor:\n",
    "        x = self.norm(x)\n",
    "        return self.layer(x, **kwargs)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" Implement Linear(d, h) -> GeLU() -> Linear(h, d) \"\"\"\n",
    "    def __init__(self, dim: int, hidden_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        return self.net(x)\n",
    "# JAX\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\" Multi-Head Self Attention \"\"\"\n",
    "    def __init__(self, dim: int, heads: int = 8, dim_head: int = 64) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        inner_dim = dim_head * heads\n",
    "        need_project_out = not (heads == 1 and dim_head == dim)\n",
    "        \n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        \n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "        \n",
    "        self.to_out = nn.Linear(inner_dim, dim) if need_project_out else nn.Identity()\n",
    "        \n",
    "    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1) # [batch_size, seq_len, (dim_head * heads)]\n",
    "        q, k , v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.heads), qkv) \n",
    "        \n",
    "        attn = torch.matmul(q, k.transpose(-1, -2)) # [batch_size, seq_len, seq_len]\n",
    "        attn = attn * self.scale\n",
    "        attn_weights = self.attend(attn)\n",
    "        \n",
    "        out = torch.matmul(attn_weights, v)             # [batch_size, heads, seq_len, dim_head]\n",
    "        out = rearrange(out, 'b h s d -> b s (h d)')    # [batch_size, seq_len, dim_head * heads == inner_dim]\n",
    "        return self.to_out(out)\n",
    "        \n",
    "    # def forward_one_head(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "    #     qkv = self.to_qkv(x) # [batch_size, seq_len, dim * 3]\n",
    "    #     qkv = qkv.chunk(3, dim=-1) # [batch_size, seq_len, dim] * 3\n",
    "    #     # qkv[0] # 0, ..., dim - 1 ||| dim, ..., 2 * dim - 1 ||| 2 * dim, ..., 3 * dim - 1\n",
    "    #     q, k, v = qkv\n",
    "        \n",
    "    #     # q [batch_size, seq_len, dim]\n",
    "    #     # k [batch_size, seq_len, dim] -> k.transpose(-1, -2) [batch_size, dim, seq_len]\n",
    "        \n",
    "    #     attn = torch.matmul(q, k.transpose(-1, -2)) # [batch_size, seq_len, seq_len]\n",
    "    #     attn_weights = self.attend(attn) # [batch_size, seq_len, seq_len]\n",
    "        \n",
    "    #     out = torch.matmul(attn_weights, v) # [batch_size, seq_len, dim]\n",
    "    #     return out\n",
    "        \n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, dim: int, depth: int, heads: int, dim_head: int, mlp_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            layer = nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads, dim_head)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim)),\n",
    "            ])\n",
    "            self.layers.append(layer)\n",
    "        \n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "            \n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "encoder = TransformerEncoder(512, 6, 8, 64, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "a = torch.randn(3, 5, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 512])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18906112"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in encoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d189d21d977b81210d15f909fc18f2c540b65e67432daf72fa2f119ed5a0108c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}