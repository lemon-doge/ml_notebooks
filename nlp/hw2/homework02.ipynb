{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Свободно переиспользуйте код с семинаров / прошлой домашки. Если копируете код из интернета, укажите ссылку откуда скопировали, это тоже можно. \n",
    "Дедлайн: 23 декабря 23:59. \n",
    "\n",
    "**ВНИМАНИЕ**: сдавать на почту `almarkv@yandex.ru` с темой письма в формате `[ML_in_SWE] Задание 2 - ФИО`. Вместо `ФИО` подставьте ваше ФИО. Если **тема** письма **не подходит** под этот формат, задание будет **считаться не сданным**.\n",
    "\n",
    "Ваш код должен выполнять на GPU, если оно доступно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from random import choices\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from einops import rearrange\n",
    "import youtokentome as yttm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Задание 1 (3 балла + возможность 1 дополнительного балла в задание 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Реализуйте TransformerDecoder\n",
    "\n",
    "Для этого вам необходимо внести изменения в код с семинара. Основные изменения (а может и все) нужно внести в AttentionLayer. Мы обсуждали их на занятии."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"homework02.jpeg\" width=\"1600\" height=\"800\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: unknown file attribute: h\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PreNorm(nn.Module):\n",
    "    \"\"\"Apply layer normalization to the input and pass it through the layer.\"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, layer: nn.Module) -> None:\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x: torch.FloatTensor, **kwargs) -> torch.FloatTensor:\n",
    "        x = self.norm(x)\n",
    "        return self.layer(x, **kwargs)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" Implement Linear(d, h) -> GeLU() -> Linear(h, d) \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, hidden_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\" Multi-Head Self Attention \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, heads: int = 8, dim_head: int = 64, mask: bool = False) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        inner_dim = dim_head * heads\n",
    "        need_project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.mask = mask\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.to_out = nn.Linear(inner_dim, dim) if need_project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)  # [batch_size, seq_len, (dim_head * heads)]\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
    "\n",
    "        attn = torch.matmul(q, k.transpose(-1, -2))  # [batch_size, seq_len, seq_len]\n",
    "        attn = attn * self.scale\n",
    "\n",
    "        if self.mask:\n",
    "            attn += torch.triu(torch.ones_like(attn), diagonal=1) * float('-inf')\n",
    "\n",
    "        attn_weights = self.attend(attn)\n",
    "\n",
    "        out = torch.matmul(attn_weights, v)  # [batch_size, heads, seq_len, dim_head]\n",
    "        out = rearrange(out, 'b h s d -> b s (h d)')  # [batch_size, seq_len, dim_head * heads == inner_dim]\n",
    "        return self.to_out(out)\n",
    "\n",
    "    # def forward_one_head(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "    #     qkv = self.to_qkv(x) # [batch_size, seq_len, dim * 3]\n",
    "    #     qkv = qkv.chunk(3, dim=-1) # [batch_size, seq_len, dim] * 3\n",
    "    #     # qkv[0] # 0, ..., dim - 1 ||| dim, ..., 2 * dim - 1 ||| 2 * dim, ..., 3 * dim - 1\n",
    "    #     q, k, v = qkv\n",
    "\n",
    "    #     # q [batch_size, seq_len, dim]\n",
    "    #     # k [batch_size, seq_len, dim] -> k.transpose(-1, -2) [batch_size, dim, seq_len]\n",
    "\n",
    "    #     attn = torch.matmul(q, k.transpose(-1, -2)) # [batch_size, seq_len, seq_len]\n",
    "    #     attn_weights = self.attend(attn) # [batch_size, seq_len, seq_len]\n",
    "\n",
    "    #     out = torch.matmul(attn_weights, v) # [batch_size, seq_len, dim]\n",
    "    #     return out\n",
    "\n",
    "# class TransformerEncoder(nn.Module):\n",
    "#     def __init__(self, dim: int, depth: int, heads: int, dim_head: int, mlp_dim: int) -> None:\n",
    "#         super().__init__()\n",
    "#         self.layers = nn.ModuleList([])\n",
    "#         for _ in range(depth):\n",
    "#             layer = nn.ModuleList([\n",
    "#                 PreNorm(dim, Attention(dim, heads, dim_head)),\n",
    "#                 PreNorm(dim, FeedForward(dim, mlp_dim)),\n",
    "#             ])\n",
    "#             self.layers.append(layer)\n",
    "#\n",
    "#         self.norm = nn.LayerNorm(dim)\n",
    "#\n",
    "#     def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "#         for attn, ff in self.layers:\n",
    "#             x = attn(x) + x\n",
    "#             x = ff(x) + x\n",
    "#\n",
    "#         return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class EthiliumDecoder(nn.Module):\n",
    "    \"\"\"Ethilium decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, dim: int = 768, nlayes: int = 3, nheads: int = 8, dim_head: int = 64,\n",
    "                 dim_hid: int = 768 * 4) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(nlayes):\n",
    "            layer = nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, nheads, dim_head, mask=True)),\n",
    "                # здесь мог быть ваш Encoder-Decoder attention, но его нет\n",
    "                PreNorm(dim, FeedForward(dim, dim_hid)),\n",
    "            ])\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "\n",
    "        return self.norm(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Задание 2 (7 баллов)\n",
    "\n",
    "Используя TransformerDecoder, реализуйте языковое моделирование на задаче сонетов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Задание 2.1 (1 балл) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Прочитайте датасет из файла sonnets.txt. Обратите внимание на то, что не все строки в файле относятся к соннетам. Предобработайте соннеты, чтобы сделать из них датасет. Датасет должен иметь следующий вид: это 4 подярд идущие строки в каком-либо сонете в нижнем регистре, такие что 4 строка оканчивается на точку, вопросительный знак или восклицательный знак. \n",
    "\n",
    "На примере первого сонета: строчки\n",
    "```\n",
    "  From fairest creatures we desire increase,\n",
    "  That thereby beauty's rose might never die,\n",
    "  But as the riper should by time decease,\n",
    "  His tender heir might bear his memory:\n",
    "```\n",
    "\n",
    "не должны быть объектом в датасете, а строчки\n",
    "\n",
    "```\n",
    "  Within thine own bud buriest thy content,\n",
    "  And tender churl mak'st waste in niggarding:\n",
    "    Pity the world, or else this glutton be,\n",
    "    To eat the world's due, by the grave and thee.\n",
    "``` \n",
    "должны (вообще, только 4 последнеи строки в 1 сонете пойдут в датасет). Строки могут пересекаться между собой, например на основе 2го сонета будет получено 2 примера в датасет: строчки 9-12 и строчки 11-14.\n",
    "\n",
    "\n",
    "После этого преобразуйте все полученные примеры в формат `@LINE1@ {line_1} @LINE2@ {line_2} @LINE3@ {line_3} @LINE4@ {line_4}`, например четверостишие выше должно иметь вид `\"@LINE1@ within thine own bud buriest thy content, @LINE2@ and tender churl mak'st waste in niggarding: @LINE3@ pity the world, or else this glutton be, @LINE4@ to eat the world's due, by the grave and thee.\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[['within thine own bud buriest thy content,',\n  \"and tender churl mak'st waste in niggarding:\",\n  '  pity the world, or else this glutton be,',\n  \"  to eat the world's due, by the grave and thee.\"],\n ['then being asked, where all thy beauty lies,',\n  'where all the treasure of thy lusty days;',\n  'to say, within thine own deep sunken eyes,',\n  'were an all-eating shame, and thriftless praise.'],\n [\"how much more praise deserv'd thy beauty's use,\",\n  \"if thou couldst answer 'this fair child of mine\",\n  \"shall sum my count, and make my old excuse,'\",\n  'proving his beauty by succession thine!'],\n [\"shall sum my count, and make my old excuse,'\",\n  'proving his beauty by succession thine!',\n  '  this were to be new made when thou art old,',\n  \"  and see thy blood warm when thou feel'st it cold.\"],\n ['look in thy glass and tell the face thou viewest',\n  'now is the time that face should form another;',\n  'whose fresh repair if now thou not renewest,',\n  'thou dost beguile the world, unbless some mother.'],\n ['whose fresh repair if now thou not renewest,',\n  'thou dost beguile the world, unbless some mother.',\n  \"for where is she so fair whose unear'd womb\",\n  'disdains the tillage of thy husbandry?'],\n [\"for where is she so fair whose unear'd womb\",\n  'disdains the tillage of thy husbandry?',\n  'or who is he so fond will be the tomb,',\n  'of his self-love to stop posterity?'],\n [\"thou art thy mother's glass and she in thee\",\n  'calls back the lovely april of her prime;',\n  'so thou through windows of thine age shalt see,',\n  'despite of wrinkles this thy golden time.'],\n ['so thou through windows of thine age shalt see,',\n  'despite of wrinkles this thy golden time.',\n  \"  but if thou live, remember'd not to be,\",\n  '  die single and thine image dies with thee.'],\n [\"nature's bequest gives nothing, but doth lend,\",\n  'and being frank she lends to those are free:',\n  'then, beauteous niggard, why dost thou abuse',\n  'the bounteous largess given thee to give?'],\n ['then, beauteous niggard, why dost thou abuse',\n  'the bounteous largess given thee to give?',\n  'profitless usurer, why dost thou use',\n  'so great a sum of sums, yet canst not live?'],\n ['for having traffic with thy self alone,',\n  'thou of thy self thy sweet self dost deceive:',\n  'then how when nature calls thee to be gone,',\n  'what acceptable audit canst thou leave?'],\n ['then how when nature calls thee to be gone,',\n  'what acceptable audit canst thou leave?',\n  '  thy unused beauty must be tombed with thee,',\n  \"  which, used, lives th' executor to be.\"],\n [\"beauty's effect with beauty were bereft,\",\n  'nor it, nor no remembrance what it was:',\n  \"  but flowers distill'd, though they with winter meet,\",\n  '  leese but their show; their substance still lives sweet.'],\n ['be wise as thou art cruel; do not press',\n  'my tongue-tied patience with too much disdain;',\n  'lest sorrow lend me words, and words express',\n  'the manner of my pity-wanting pain.'],\n ['for, if i should despair, i should grow mad,',\n  'and in my madness might speak ill of thee;',\n  'now this ill-wresting world is grown so bad,',\n  'mad slanderers by mad ears believed be.'],\n ['now this ill-wresting world is grown so bad,',\n  'mad slanderers by mad ears believed be.',\n  '  that i may not be so, nor thou belied,',\n  '  bear thine eyes straight, though thy proud heart go wide.'],\n ['in faith i do not love thee with mine eyes,',\n  'for they in thee a thousand errors note;',\n  \"but 'tis my heart that loves what they despise,\",\n  'who, in despite of view, is pleased to dote.'],\n [\"who leaves unsway'd the likeness of a man,\",\n  \"thy proud heart's slave and vassal wretch to be:\",\n  '  only my plague thus far i count my gain,',\n  '  that she that makes me sin awards me pain.'],\n ['or, if it do, not from those lips of thine,',\n  \"that have profan'd their scarlet ornaments\",\n  \"and seal'd false bonds of love as oft as mine,\",\n  \"robb'd others' beds' revenues of their rents.\"],\n [\"be it lawful i love thee, as thou lov'st those\",\n  'whom thine eyes woo as mine importune thee:',\n  'root pity in thy heart, that, when it grows,',\n  'thy pity may deserve to pitied be.'],\n ['root pity in thy heart, that, when it grows,',\n  'thy pity may deserve to pitied be.',\n  '  if thou dost seek to have what thou dost hide,',\n  '  by self-example mayst thou be denied!'],\n ['but if thou catch thy hope, turn back to me,',\n  \"and play the mother's part, kiss me, be kind;\",\n  \"  so will i pray that thou mayst have thy 'will,'\",\n  '  if thou turn back and my loud crying still.'],\n ['two loves i have of comfort and despair,',\n  'which like two spirits do suggest me still:',\n  'the better angel is a man right fair,',\n  \"the worser spirit a woman colour'd ill.\"],\n ['to win me soon to hell, my female evil,',\n  'tempteth my better angel from my side,',\n  'and would corrupt my saint to be a devil,',\n  'wooing his purity with her foul pride.'],\n ['but being both from me, both to each friend,',\n  \"i guess one angel in another's hell:\",\n  \"  yet this shall i ne'er know, but live in doubt,\",\n  '  till my bad angel fire my good one out.'],\n [\"'i hate' she alter'd with an end,\",\n  'that followed it as gentle day,',\n  'doth follow night, who like a fiend',\n  'from heaven to hell is flown away.'],\n ['doth follow night, who like a fiend',\n  'from heaven to hell is flown away.',\n  \"  'i hate', from hate away she threw,\",\n  \"  and sav'd my life, saying 'not you'.\"],\n ['poor soul, the centre of my sinful earth,',\n  'my sinful earth these rebel powers array,',\n  'why dost thou pine within and suffer dearth,',\n  'painting thy outward walls so costly gay?'],\n ['why dost thou pine within and suffer dearth,',\n  'painting thy outward walls so costly gay?',\n  'why so large cost, having so short a lease,',\n  'dost thou upon thy fading mansion spend?'],\n ['why so large cost, having so short a lease,',\n  'dost thou upon thy fading mansion spend?',\n  'shall worms, inheritors of this excess,',\n  \"eat up thy charge? is this thy body's end?\"],\n ['buy terms divine in selling hours of dross;',\n  'within be fed, without be rich no more:',\n  '  so shall thou feed on death, that feeds on men,',\n  \"  and death once dead, there's no more dying then.\"],\n ['my love is as a fever longing still,',\n  'for that which longer nurseth the disease;',\n  'feeding on that which doth preserve the ill,',\n  'the uncertain sickly appetite to please.'],\n ['my reason, the physician to my love,',\n  'angry that his prescriptions are not kept,',\n  'hath left me, and i desperate now approve',\n  'desire is death, which physic did except.'],\n [\"my thoughts and my discourse as madmen's are,\",\n  \"at random from the truth vainly express'd;\",\n  '  for i have sworn thee fair, and thought thee bright,',\n  '  who art as black as hell, as dark as night.'],\n ['o me! what eyes hath love put in my head,',\n  'which have no correspondence with true sight;',\n  'or, if they have, where is my judgment fled,',\n  'that censures falsely what they see aright?'],\n ['or, if they have, where is my judgment fled,',\n  'that censures falsely what they see aright?',\n  'if that be fair whereon my false eyes dote,',\n  'what means the world to say it is not so?'],\n ['if it be not, then love doth well denote',\n  \"love's eye is not so true as all men's: no,\",\n  \"how can it? o! how can love's eye be true,\",\n  'that is so vexed with watching and with tears?'],\n [\"how can it? o! how can love's eye be true,\",\n  'that is so vexed with watching and with tears?',\n  'no marvel then, though i mistake my view;',\n  'the sun itself sees not, till heaven clears.'],\n ['no marvel then, though i mistake my view;',\n  'the sun itself sees not, till heaven clears.',\n  \"  o cunning love! with tears thou keep'st me blind,\",\n  '  lest eyes well-seeing thy foul faults should find.'],\n ['canst thou, o cruel! say i love thee not,',\n  'when i against myself with thee partake?',\n  'do i not think on thee, when i forgot',\n  'am of my self, all tyrant, for thy sake?'],\n ['who hateth thee that i do call my friend,',\n  \"on whom frown'st thou that i do fawn upon,\",\n  \"nay, if thou lour'st on me, do i not spend\",\n  'revenge upon myself with present moan?'],\n ['what merit do i in my self respect,',\n  'that is so proud thy service to despise,',\n  'when all my best doth worship thy defect,',\n  'commanded by the motion of thine eyes?'],\n ['when all my best doth worship thy defect,',\n  'commanded by the motion of thine eyes?',\n  '  but, love, hate on, for now i know thy mind;',\n  \"  those that can see thou lov'st, and i am blind.\"],\n ['o! from what power hast thou this powerful might,',\n  'with insufficiency my heart to sway?',\n  'to make me give the lie to my true sight,',\n  'and swear that brightness doth not grace the day?'],\n ['whence hast thou this becoming of things ill,',\n  'that in the very refuse of thy deeds',\n  'there is such strength and warrantise of skill,',\n  'that, in my mind, thy worst all best exceeds?'],\n ['there is such strength and warrantise of skill,',\n  'that, in my mind, thy worst all best exceeds?',\n  'who taught thee how to make me love thee more,',\n  'the more i hear and see just cause of hate?'],\n ['o! though i love what others do abhor,',\n  'with others thou shouldst not abhor my state:',\n  \"  if thy unworthiness rais'd love in me,\",\n  \"  more worthy i to be belov'd of thee.\"],\n ['but rising at thy name doth point out thee,',\n  'as his triumphant prize. proud of this pride,',\n  'he is contented thy poor drudge to be,',\n  'to stand in thy affairs, fall by thy side.'],\n ['he is contented thy poor drudge to be,',\n  'to stand in thy affairs, fall by thy side.',\n  '  no want of conscience hold it that i call',\n  \"  her 'love,' for whose dear love i rise and fall.\"],\n ['and, to enlighten thee, gave eyes to blindness,',\n  'or made them swear against the thing they see;',\n  \"  for i have sworn thee fair; more perjur'd i,\",\n  '  to swear against the truth so foul a lie!'],\n [\"which borrow'd from this holy fire of love,\",\n  'a dateless lively heat, still to endure,',\n  'and grew a seeting bath, which yet men prove',\n  'against strange maladies a sovereign cure.'],\n ['i, sick withal, the help of bath desired,',\n  \"and thither hied, a sad distemper'd guest,\",\n  '  but found no cure, the bath for my help lies',\n  \"  where cupid got new fire; my mistress' eyes.\"]]"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists('sonnets.txt'):\n",
    "    !wget https: // raw.githubusercontent.com / girafe-ai / ml-course / master / homeworks_basic / Lab2_DL / sonnets.txt\n",
    "\n",
    "with open('sonnets.txt', 'r') as iofile:\n",
    "    text = iofile.readlines()\n",
    "\n",
    "TEXT_START = 45\n",
    "TEXT_END = -368\n",
    "text = text[TEXT_START: TEXT_END]\n",
    "headline_indices = [0]\n",
    "\n",
    "for i in range(len(text)):\n",
    "    if text[i] == '\\n':\n",
    "        headline_indices.append(i)\n",
    "    text[i] = text[i].lower()[2::]\n",
    "\n",
    "paragraphs = []\n",
    "\n",
    "for i in range(0, len(headline_indices) - 1, 2):\n",
    "    l, r = headline_indices[i], headline_indices[i + 1]\n",
    "    paragraphs.append(''.join(text[l:r]))\n",
    "\n",
    "paragraphs = [p.split('\\n')[:-1] for p in paragraphs]\n",
    "\n",
    "suitable_paragraphs = []\n",
    "for p in paragraphs:\n",
    "    for i in range(3, len(p)):\n",
    "        if p[i].endswith(('.', '?', '!')):\n",
    "            suitable_paragraphs.append(p[i - 3:i + 1])\n",
    "\n",
    "suitable_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "[\"@LINE1@ within thine own bud buriest thy content, @LINE2@ and tender churl mak'st waste in niggarding: @LINE3@ pity the world, or else this glutton be, @LINE4@ to eat the world's due, by the grave and thee.\",\n '@LINE1@ then being asked, where all thy beauty lies, @LINE2@ where all the treasure of thy lusty days; @LINE3@ to say, within thine own deep sunken eyes, @LINE4@ were an all-eating shame, and thriftless praise.',\n \"@LINE1@ how much more praise deserv'd thy beauty's use, @LINE2@ if thou couldst answer 'this fair child of mine @LINE3@ shall sum my count, and make my old excuse,' @LINE4@ proving his beauty by succession thine!\",\n \"@LINE1@ shall sum my count, and make my old excuse,' @LINE2@ proving his beauty by succession thine! @LINE3@ this were to be new made when thou art old, @LINE4@ and see thy blood warm when thou feel'st it cold.\",\n '@LINE1@ look in thy glass and tell the face thou viewest @LINE2@ now is the time that face should form another; @LINE3@ whose fresh repair if now thou not renewest, @LINE4@ thou dost beguile the world, unbless some mother.',\n \"@LINE1@ whose fresh repair if now thou not renewest, @LINE2@ thou dost beguile the world, unbless some mother. @LINE3@ for where is she so fair whose unear'd womb @LINE4@ disdains the tillage of thy husbandry?\",\n \"@LINE1@ for where is she so fair whose unear'd womb @LINE2@ disdains the tillage of thy husbandry? @LINE3@ or who is he so fond will be the tomb, @LINE4@ of his self-love to stop posterity?\",\n \"@LINE1@ thou art thy mother's glass and she in thee @LINE2@ calls back the lovely april of her prime; @LINE3@ so thou through windows of thine age shalt see, @LINE4@ despite of wrinkles this thy golden time.\",\n \"@LINE1@ so thou through windows of thine age shalt see, @LINE2@ despite of wrinkles this thy golden time. @LINE3@ but if thou live, remember'd not to be, @LINE4@ die single and thine image dies with thee.\",\n \"@LINE1@ nature's bequest gives nothing, but doth lend, @LINE2@ and being frank she lends to those are free: @LINE3@ then, beauteous niggard, why dost thou abuse @LINE4@ the bounteous largess given thee to give?\",\n '@LINE1@ then, beauteous niggard, why dost thou abuse @LINE2@ the bounteous largess given thee to give? @LINE3@ profitless usurer, why dost thou use @LINE4@ so great a sum of sums, yet canst not live?',\n '@LINE1@ for having traffic with thy self alone, @LINE2@ thou of thy self thy sweet self dost deceive: @LINE3@ then how when nature calls thee to be gone, @LINE4@ what acceptable audit canst thou leave?',\n \"@LINE1@ then how when nature calls thee to be gone, @LINE2@ what acceptable audit canst thou leave? @LINE3@ thy unused beauty must be tombed with thee, @LINE4@ which, used, lives th' executor to be.\",\n \"@LINE1@ beauty's effect with beauty were bereft, @LINE2@ nor it, nor no remembrance what it was: @LINE3@ but flowers distill'd, though they with winter meet, @LINE4@ leese but their show; their substance still lives sweet.\",\n '@LINE1@ be wise as thou art cruel; do not press @LINE2@ my tongue-tied patience with too much disdain; @LINE3@ lest sorrow lend me words, and words express @LINE4@ the manner of my pity-wanting pain.',\n '@LINE1@ for, if i should despair, i should grow mad, @LINE2@ and in my madness might speak ill of thee; @LINE3@ now this ill-wresting world is grown so bad, @LINE4@ mad slanderers by mad ears believed be.',\n '@LINE1@ now this ill-wresting world is grown so bad, @LINE2@ mad slanderers by mad ears believed be. @LINE3@ that i may not be so, nor thou belied, @LINE4@ bear thine eyes straight, though thy proud heart go wide.',\n \"@LINE1@ in faith i do not love thee with mine eyes, @LINE2@ for they in thee a thousand errors note; @LINE3@ but 'tis my heart that loves what they despise, @LINE4@ who, in despite of view, is pleased to dote.\",\n \"@LINE1@ who leaves unsway'd the likeness of a man, @LINE2@ thy proud heart's slave and vassal wretch to be: @LINE3@ only my plague thus far i count my gain, @LINE4@ that she that makes me sin awards me pain.\",\n \"@LINE1@ or, if it do, not from those lips of thine, @LINE2@ that have profan'd their scarlet ornaments @LINE3@ and seal'd false bonds of love as oft as mine, @LINE4@ robb'd others' beds' revenues of their rents.\",\n \"@LINE1@ be it lawful i love thee, as thou lov'st those @LINE2@ whom thine eyes woo as mine importune thee: @LINE3@ root pity in thy heart, that, when it grows, @LINE4@ thy pity may deserve to pitied be.\",\n '@LINE1@ root pity in thy heart, that, when it grows, @LINE2@ thy pity may deserve to pitied be. @LINE3@ if thou dost seek to have what thou dost hide, @LINE4@ by self-example mayst thou be denied!',\n \"@LINE1@ but if thou catch thy hope, turn back to me, @LINE2@ and play the mother's part, kiss me, be kind; @LINE3@ so will i pray that thou mayst have thy 'will,' @LINE4@ if thou turn back and my loud crying still.\",\n \"@LINE1@ two loves i have of comfort and despair, @LINE2@ which like two spirits do suggest me still: @LINE3@ the better angel is a man right fair, @LINE4@ the worser spirit a woman colour'd ill.\",\n '@LINE1@ to win me soon to hell, my female evil, @LINE2@ tempteth my better angel from my side, @LINE3@ and would corrupt my saint to be a devil, @LINE4@ wooing his purity with her foul pride.',\n \"@LINE1@ but being both from me, both to each friend, @LINE2@ i guess one angel in another's hell: @LINE3@ yet this shall i ne'er know, but live in doubt, @LINE4@ till my bad angel fire my good one out.\",\n \"@LINE1@ 'i hate' she alter'd with an end, @LINE2@ that followed it as gentle day, @LINE3@ doth follow night, who like a fiend @LINE4@ from heaven to hell is flown away.\",\n \"@LINE1@ doth follow night, who like a fiend @LINE2@ from heaven to hell is flown away. @LINE3@ 'i hate', from hate away she threw, @LINE4@ and sav'd my life, saying 'not you'.\",\n '@LINE1@ poor soul, the centre of my sinful earth, @LINE2@ my sinful earth these rebel powers array, @LINE3@ why dost thou pine within and suffer dearth, @LINE4@ painting thy outward walls so costly gay?',\n '@LINE1@ why dost thou pine within and suffer dearth, @LINE2@ painting thy outward walls so costly gay? @LINE3@ why so large cost, having so short a lease, @LINE4@ dost thou upon thy fading mansion spend?',\n \"@LINE1@ why so large cost, having so short a lease, @LINE2@ dost thou upon thy fading mansion spend? @LINE3@ shall worms, inheritors of this excess, @LINE4@ eat up thy charge? is this thy body's end?\",\n \"@LINE1@ buy terms divine in selling hours of dross; @LINE2@ within be fed, without be rich no more: @LINE3@ so shall thou feed on death, that feeds on men, @LINE4@ and death once dead, there's no more dying then.\",\n '@LINE1@ my love is as a fever longing still, @LINE2@ for that which longer nurseth the disease; @LINE3@ feeding on that which doth preserve the ill, @LINE4@ the uncertain sickly appetite to please.',\n '@LINE1@ my reason, the physician to my love, @LINE2@ angry that his prescriptions are not kept, @LINE3@ hath left me, and i desperate now approve @LINE4@ desire is death, which physic did except.',\n \"@LINE1@ my thoughts and my discourse as madmen's are, @LINE2@ at random from the truth vainly express'd; @LINE3@ for i have sworn thee fair, and thought thee bright, @LINE4@ who art as black as hell, as dark as night.\",\n '@LINE1@ o me! what eyes hath love put in my head, @LINE2@ which have no correspondence with true sight; @LINE3@ or, if they have, where is my judgment fled, @LINE4@ that censures falsely what they see aright?',\n '@LINE1@ or, if they have, where is my judgment fled, @LINE2@ that censures falsely what they see aright? @LINE3@ if that be fair whereon my false eyes dote, @LINE4@ what means the world to say it is not so?',\n \"@LINE1@ if it be not, then love doth well denote @LINE2@ love's eye is not so true as all men's: no, @LINE3@ how can it? o! how can love's eye be true, @LINE4@ that is so vexed with watching and with tears?\",\n \"@LINE1@ how can it? o! how can love's eye be true, @LINE2@ that is so vexed with watching and with tears? @LINE3@ no marvel then, though i mistake my view; @LINE4@ the sun itself sees not, till heaven clears.\",\n \"@LINE1@ no marvel then, though i mistake my view; @LINE2@ the sun itself sees not, till heaven clears. @LINE3@ o cunning love! with tears thou keep'st me blind, @LINE4@ lest eyes well-seeing thy foul faults should find.\",\n '@LINE1@ canst thou, o cruel! say i love thee not, @LINE2@ when i against myself with thee partake? @LINE3@ do i not think on thee, when i forgot @LINE4@ am of my self, all tyrant, for thy sake?',\n \"@LINE1@ who hateth thee that i do call my friend, @LINE2@ on whom frown'st thou that i do fawn upon, @LINE3@ nay, if thou lour'st on me, do i not spend @LINE4@ revenge upon myself with present moan?\",\n '@LINE1@ what merit do i in my self respect, @LINE2@ that is so proud thy service to despise, @LINE3@ when all my best doth worship thy defect, @LINE4@ commanded by the motion of thine eyes?',\n \"@LINE1@ when all my best doth worship thy defect, @LINE2@ commanded by the motion of thine eyes? @LINE3@ but, love, hate on, for now i know thy mind; @LINE4@ those that can see thou lov'st, and i am blind.\",\n '@LINE1@ o! from what power hast thou this powerful might, @LINE2@ with insufficiency my heart to sway? @LINE3@ to make me give the lie to my true sight, @LINE4@ and swear that brightness doth not grace the day?',\n '@LINE1@ whence hast thou this becoming of things ill, @LINE2@ that in the very refuse of thy deeds @LINE3@ there is such strength and warrantise of skill, @LINE4@ that, in my mind, thy worst all best exceeds?',\n '@LINE1@ there is such strength and warrantise of skill, @LINE2@ that, in my mind, thy worst all best exceeds? @LINE3@ who taught thee how to make me love thee more, @LINE4@ the more i hear and see just cause of hate?',\n \"@LINE1@ o! though i love what others do abhor, @LINE2@ with others thou shouldst not abhor my state: @LINE3@ if thy unworthiness rais'd love in me, @LINE4@ more worthy i to be belov'd of thee.\",\n '@LINE1@ but rising at thy name doth point out thee, @LINE2@ as his triumphant prize. proud of this pride, @LINE3@ he is contented thy poor drudge to be, @LINE4@ to stand in thy affairs, fall by thy side.',\n \"@LINE1@ he is contented thy poor drudge to be, @LINE2@ to stand in thy affairs, fall by thy side. @LINE3@ no want of conscience hold it that i call @LINE4@ her 'love,' for whose dear love i rise and fall.\",\n \"@LINE1@ and, to enlighten thee, gave eyes to blindness, @LINE2@ or made them swear against the thing they see; @LINE3@ for i have sworn thee fair; more perjur'd i, @LINE4@ to swear against the truth so foul a lie!\",\n \"@LINE1@ which borrow'd from this holy fire of love, @LINE2@ a dateless lively heat, still to endure, @LINE3@ and grew a seeting bath, which yet men prove @LINE4@ against strange maladies a sovereign cure.\",\n \"@LINE1@ i, sick withal, the help of bath desired, @LINE2@ and thither hied, a sad distemper'd guest, @LINE3@ but found no cure, the bath for my help lies @LINE4@ where cupid got new fire; my mistress' eyes.\"]"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [f\"@LINE1@ {p[0]} @LINE2@ {p[1]} @LINE3@ {p[2].strip()} @LINE4@ {p[3].strip()}\" for p in suitable_paragraphs]\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "assert \"@LINE1@ within thine own bud buriest thy content, @LINE2@ and tender churl mak'st waste in niggarding: @LINE3@ pity the world, or else this glutton be, @LINE4@ to eat the world's due, by the grave and thee.\" == data[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Задание 2.2 (1 балл)\n",
    "\n",
    "Обучите токенизатор BPE на полученном датасете  (воспользуйтесь библиотекой [YTTM](https://github.com/VKCOM/YouTokenToMe)), проверье что `@LINE1@`, `@LINE2@`, `@LINE3@`, `@LINE4@` представлены в виде одного токена. Используйте `vocab_size = 6000`.  \n",
    "\n",
    "Токенизируйте все примеры в датасете, составьте Vocabulary, проверьте что у вас получается преобразовывать токенизированные примеры в `input_ids` для deep learning (этот код можно скопировать из прошлой домашки + семинаров)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('data.txt', 'w') as file:\n",
    "    file.writelines(' '.join(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training parameters\n",
      "  input: data.txt\n",
      "  model: model.model\n",
      "  vocab_size: 6000\n",
      "  n_threads: 8\n",
      "  character_coverage: 1\n",
      "  pad: 0\n",
      "  unk: 1\n",
      "  bos: 2\n",
      "  eos: 3\n",
      "\n",
      "reading file...\n",
      "learning bpe...\n",
      "number of unique characters in the training data: 43\n",
      "number of deleted characters: 0\n",
      "number of unique characters left: 43\n",
      "id: 1000=55+612               freq: 1           subword: ▁sake?=▁s+ake?\n",
      "WARNING merged only: 1495 pairs of tokens\n",
      "model saved to: model.model\n"
     ]
    }
   ],
   "source": [
    "# Training model\n",
    "yttm.BPE.train(data='data.txt', vocab_size=6000, model='model.model')\n",
    "\n",
    "# Loading model\n",
    "bpe = yttm.BPE(model='model.model')\n",
    "\n",
    "tokenized = bpe.encode(data, bos=True, eos=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "outputs": [
    {
     "data": {
      "text/plain": "[\"<BOS> @LINE1@ within thine own bud buriest thy content, @LINE2@ and tender churl mak'st waste in niggarding: @LINE3@ pity the world, or else this glutton be, @LINE4@ to eat the world's due, by the grave and thee.<EOS>\"]"
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe.decode(tokenized[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "outputs": [
    {
     "data": {
      "text/plain": "['<PAD>', '<UNK>', '<BOS>', '<EOS>', '▁']"
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe.vocab()[:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "outputs": [],
   "source": [
    "#padding\n",
    "max_len = max([len(t) for t in tokenized])\n",
    "for i in range(len(tokenized)):\n",
    "    while len(tokenized[i]) < max_len:\n",
    "        tokenized[i].append(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([53, 44])"
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padd_tokens_tensor = torch.LongTensor(tokenized)\n",
    "padd_tokens_tensor.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomTokenDataset(Dataset):\n",
    "    def __init__(self, padd_tokens_tensor):\n",
    "        self.padd_tokens_tensor = padd_tokens_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.padd_tokens_tensor)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return padd_tokens_tensor[idx], padd_tokens_tensor[idx, :-1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "outputs": [],
   "source": [
    "train_dataset = CustomTokenDataset(padd_tokens_tensor[:37])\n",
    "val_dataset = CustomTokenDataset(padd_tokens_tensor[37:])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 12\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([12, 43])"
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(val_dataloader))[1].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "outputs": [],
   "source": [
    "# def get_batch(data, idx, batch_size = 12):\n",
    "#     choosen_idx = random.sample(range(padd_tokens_tensor.shape[0]), batch_size)\n",
    "#     input = padd_tokens_tensor[choosen_idx].transpose(1,0)\n",
    "#     target = padd_tokens_tensor[choosen_idx , :-1].transpose(1,0)\n",
    "#     return input, target"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "outputs": [],
   "source": [
    "# def get_batch(index: int):\n",
    "#     if index >= padd_tokens_tensor.shape[1]:\n",
    "#         raise IndexError(\"what's up?\")\n",
    "#     input = padd_tokens_tensor[:, :index]\n",
    "#     target = padd_tokens_tensor[:, index].type(torch.LongTensor)\n",
    "#     return input, target"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([44, 12])"
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_batch()[0].shape # [seq_len, batch_size]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "outputs": [
    {
     "data": {
      "text/plain": "1495"
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe.vocab_size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"float\") to list",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [131]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m bpe\u001B[38;5;241m.\u001B[39mencode(\u001B[43mchoices\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbpe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvocab\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrand\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1495\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m53\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtolist\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m/usr/local/Cellar/python@3.9/3.9.7/Frameworks/Python.framework/Versions/3.9/lib/python3.9/random.py:500\u001B[0m, in \u001B[0;36mRandom.choices\u001B[0;34m(self, population, weights, cum_weights, k)\u001B[0m\n\u001B[1;32m    498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(cum_weights) \u001B[38;5;241m!=\u001B[39m n:\n\u001B[1;32m    499\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mThe number of weights does not match the population\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m--> 500\u001B[0m total \u001B[38;5;241m=\u001B[39m \u001B[43mcum_weights\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.0\u001B[39;49m   \u001B[38;5;66;03m# convert to float\u001B[39;00m\n\u001B[1;32m    501\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m total \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m:\n\u001B[1;32m    502\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTotal of weights must be greater than zero\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mTypeError\u001B[0m: can only concatenate list (not \"float\") to list"
     ]
    }
   ],
   "source": [
    "# bpe.encode(choices(bpe.vocab(), weights=torch.rand(1495,1,53).tolist(), k = 1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Задание 2.3 (3 балла + 1 бонус)\n",
    "\n",
    "Реализуйте языковую модель на основе TransformerDecoder-а. Если Вы не выполнили задание выше, используйте `torch.nn.TransformerDecoder`. Если вы выполнили первое задание, вы получите 1 дополнительный балл за использование собственного TransformerDecoder-а."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "# нам понадобится этот пацан, чтобы че-то заработало\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/main/Desktop/python projects/jupyter/venv/lib/python3.9/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "# https://jalammar.github.io/illustrated-gpt2/\n",
    "# https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "class EthanolDecepticon(nn.Module):\n",
    "    def __init__(self, vocab_size: int, dim_model: int = 768, nlayers: int = 3, nheads: int = 8, dim_head: int = 64,\n",
    "                 dim_hid: int = 768 * 4, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim_model = dim_model\n",
    "        self.nlayers = nlayers\n",
    "        self.nheads = nheads\n",
    "        self.dim_head = dim_head\n",
    "        self.dim_hid = dim_hid\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embed = nn.Embedding(num_embeddings=vocab_size, embedding_dim=dim_model)\n",
    "        self.pos_encoder = PositionalEncoding(dim_model, dropout)\n",
    "        self.decoder_layers = EthiliumDecoder(dim_model, nlayers, nheads, dim_head, dim_hid)\n",
    "        # todo: add fc + softmax\n",
    "        self.fc = nn.LazyLinear(vocab_size)\n",
    "        self.softmax = nn.Softmax(dim = 0)\n",
    "        # encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        # self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        # self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        # self.d_model = d_model\n",
    "        # self.decoder = nn.Linear(d_model, ntoken)\n",
    "\n",
    "    #     self.init_weights()\n",
    "    #\n",
    "    # def init_weights(self) -> None:\n",
    "    #     initrange = 0.1\n",
    "    #     self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "    #     self.decoder.bias.data.zero_()\n",
    "    #     self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    # takes tokenized input\n",
    "    # returns probability distribution vector\n",
    "    def forward(self, src: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, vocab_size]\n",
    "        \"\"\"\n",
    "        src = self.embed(src) * math.sqrt(self.dim_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        src = self.decoder_layers(src)\n",
    "        src = self.fc(src)\n",
    "        return self.softmax(src)\n",
    "\n",
    "\n",
    "model = EthanolDecepticon(vocab_size=bpe.vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         ...,\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan]],\n\n        [[nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         ...,\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan]],\n\n        [[nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         ...,\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan]],\n\n        ...,\n\n        [[nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         ...,\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan]],\n\n        [[nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         ...,\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan]],\n\n        [[nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         ...,\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan],\n         [nan, nan, nan,  ..., nan, nan, nan]]], grad_fn=<SoftmaxBackward0>)"
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(next(iter(val_dataloader))[0].transpose(1,0)) # transpose!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (528) to match target batch_size (516).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [306]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28minput\u001B[39m, target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28miter\u001B[39m(val_dataloader))\n\u001B[1;32m      3\u001B[0m pred \u001B[38;5;241m=\u001B[39m model(\u001B[38;5;28minput\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mcriterion\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpred\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mview\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvocab_size\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mview\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m loss\n",
      "File \u001B[0;32m~/Desktop/python projects/jupyter/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/python projects/jupyter/venv/lib/python3.9/site-packages/torch/nn/modules/loss.py:1164\u001B[0m, in \u001B[0;36mCrossEntropyLoss.forward\u001B[0;34m(self, input, target)\u001B[0m\n\u001B[1;32m   1163\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m-> 1164\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1165\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1166\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/python projects/jupyter/venv/lib/python3.9/site-packages/torch/nn/functional.py:3014\u001B[0m, in \u001B[0;36mcross_entropy\u001B[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001B[0m\n\u001B[1;32m   3012\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   3013\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[0;32m-> 3014\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_Reduction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_enum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mValueError\u001B[0m: Expected input batch_size (528) to match target batch_size (516)."
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "input, target = next(iter(val_dataloader))\n",
    "pred = model(input)\n",
    "loss = criterion(pred.view(-1, model.vocab_size), target.view(-1))\n",
    "loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Задание 2.4 (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Обучите модель. Изобразите как меняется лосс, сгенерируйте несколько примеров стихов. Баллы за качество генерации снижаться не будут, но лосс должен понижаться + должна быть функция для получения случайного стиха."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "def train(nepochs: int, model: nn.Module) -> None:\n",
    "    # log_interval = 200\n",
    "\n",
    "    for epoch in range(nepochs):\n",
    "        # total_loss = 0.\n",
    "        model.train()\n",
    "        for input, target in train_dataloader:\n",
    "            pred = model(input)\n",
    "\n",
    "            loss = criterion(pred.view(-1, model.vocab_size), target.view(-1)) # вообще, так неправильно, но уже ладно\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            if i % log_interval == 0 and i > 0:\n",
    "                lr = scheduler.get_last_lr()[0]\n",
    "                loss_interval_mean = total_loss / log_interval\n",
    "                print(f'| epoch {epoch:3d} | {i:5d}/{768:5d} batches | '\n",
    "                      f'lr {lr:02.2f} | '\n",
    "                      f'loss mean {loss_interval_mean:5.2f} | '\n",
    "                      f'current batch loss {loss}')\n",
    "                total_loss = 0\n",
    "\n",
    "        # total_loss = 0.\n",
    "        model.eval()\n",
    "        for input, target in val_dataloader:\n",
    "            pred = model(input)\n",
    "\n",
    "            loss = criterion(pred, target) # вообще, так неправильно, но уже ладно\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            if i % log_interval == 0 and i > 0:\n",
    "                lr = scheduler.get_last_lr()[0]\n",
    "                loss_interval_mean = total_loss / log_interval\n",
    "                print(f'| epoch {epoch:3d} | {i:5d}/{768:5d} batches | '\n",
    "                      f'lr {lr:02.2f} | '\n",
    "                      f'loss mean {loss_interval_mean:5.2f} | '\n",
    "                      f'current batch loss {loss}')\n",
    "                total_loss = 0\n",
    "\n",
    "    scheduler.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/768 [00:00<?, ?it/s]/var/folders/3f/zdr9bx9916d5985j006hy68c0000gn/T/ipykernel_1208/2450098259.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.softmax(output)\n",
      "  0%|          | 0/768 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [53, 1495], got [53]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [162]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [161]\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(nepochs, model)\u001B[0m\n\u001B[1;32m     17\u001B[0m x, target \u001B[38;5;241m=\u001B[39m get_batch(i)\n\u001B[1;32m     18\u001B[0m pred \u001B[38;5;241m=\u001B[39m model(x)\n\u001B[0;32m---> 19\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mcriterion\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# вообще, так неправильно, но уже ладно\u001B[39;00m\n\u001B[1;32m     21\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     22\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/Desktop/python projects/jupyter/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/python projects/jupyter/venv/lib/python3.9/site-packages/torch/nn/modules/loss.py:1164\u001B[0m, in \u001B[0;36mCrossEntropyLoss.forward\u001B[0;34m(self, input, target)\u001B[0m\n\u001B[1;32m   1163\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m-> 1164\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1165\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1166\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/python projects/jupyter/venv/lib/python3.9/site-packages/torch/nn/functional.py:3014\u001B[0m, in \u001B[0;36mcross_entropy\u001B[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001B[0m\n\u001B[1;32m   3012\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   3013\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[0;32m-> 3014\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_Reduction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_enum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Expected target size [53, 1495], got [53]"
     ]
    }
   ],
   "source": [
    "train(1, model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_loss(predictions: torch.Tensor, answers):\n",
    "\n",
    "\n",
    "# YOUR CODE GOES HERE\n",
    "\n",
    "def traininng_step(model, optimizer, inputs):\n",
    "    \"\"\"\n",
    "    This function performs a training step.\n",
    "    param: model -- SonetModel object\n",
    "    param: optimizer -- torch.optim object\n",
    "    param: inputs -- output of torch DataLoader, one batch \n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def train(model, optimizer, train_loader, val_loader, num_epochs=10):\n",
    "    \"\"\"\n",
    "    This function performs a training loop.\n",
    "    param: model -- SonetModel object\n",
    "    param: optimizer -- torch.optim object\n",
    "    param: train_loader -- torch DataLoader object\n",
    "    param: val_loader -- torch DataLoader object\n",
    "    param: num_epochs -- number of epochs\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Обратите внимание, что специальные токены были нужны нам чтобы после генерации мы могли разложить стихи на четверостишия обратно и красиво его нариовать (ну и чтобы моделе помочь выучить рифму/ритм на самом деле, но для этого нужны большие трансформеры)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d189d21d977b81210d15f909fc18f2c540b65e67432daf72fa2f119ed5a0108c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}